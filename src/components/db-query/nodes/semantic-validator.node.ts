import {PromptTemplate} from '@langchain/core/prompts';
import {RunnableSequence} from '@langchain/core/runnables';
import {LangGraphRunnableConfig} from '@langchain/langgraph';
import {inject} from '@loopback/context';
import {service} from '@loopback/core';
import {graphNode} from '../../../decorators';
import {IGraphNode, LLMStreamEventType} from '../../../graphs';
import {AiIntegrationBindings} from '../../../keys';
import {LLMProvider} from '../../../types';
import {stripThinkingTokens} from '../../../utils';
import {DbQueryAIExtensionBindings} from '../keys';
import {DbQueryNodes} from '../nodes.enum';
import {DbSchemaHelperService} from '../services';
import {DbQueryState} from '../state';
import {DbQueryConfig, EvaluationResult} from '../types';

@graphNode(DbQueryNodes.SemanticValidator)
export class SemanticValidatorNode implements IGraphNode<DbQueryState> {
  constructor(
    @inject(AiIntegrationBindings.SmartLLM)
    private readonly smartllm: LLMProvider,
    @inject(AiIntegrationBindings.CheapLLM)
    private readonly cheapllm: LLMProvider,
    @inject(DbQueryAIExtensionBindings.Config)
    private readonly config: DbQueryConfig,
    @service(DbSchemaHelperService)
    private readonly schemaHelper: DbSchemaHelperService,
    @inject(DbQueryAIExtensionBindings.GlobalContext, {optional: true})
    private readonly checks?: string[],
  ) {}

  prompt = PromptTemplate.fromTemplate(`
<instructions>
You are an AI assistant that judges whether the generated and syntactically verified SQL query will satisfy the user's query and the additional checks provided.
The query has already been validated for syntax and correctness, so you only need to check if it satisfies the user's query and all the additional checks provided.
DO NOT check for syntax issues as the query is confirmed to run correctly on the database. only the relevant or correctness of results is to be checked.
DO NOT make up issues or flaws that do not exist in the query, or reporting missing checks that are not actually missing.
You must create a checklist and ensure that the query satisfies all the points in the checklist.
</instructions>

<latest-query>
{query}
</latest-query>

<user-question>
{prompt}
</user-question>

<database-schema>
{schema}
</database-schema>

{checks}

{feedbacks}

<output-instructions>
The last line of your response must be either 'valid' or 'invalid'.
In case of 'invalid', you must provide the reasons for invalidity after a colon and space.
The format in case of invalid query should be -

invalid: reason for invalidity

The format in case of valid query should just be the string 'valid' with no other explanation or string, the output should just be -

valid

</output-instructions>
`);

  feedbackPrompt = PromptTemplate.fromTemplate(`
<feedback-instructions>
We also need to consider the users feedback on the last attempt at query generation.

But was rejected by validator with the following errors -
{feedback}

Keep these feedbacks in mind while validating the new query.
</feedback-instructions>`);

  async execute(
    state: DbQueryState,
    config: LangGraphRunnableConfig,
  ): Promise<DbQueryState> {
    config.writer?.({
      type: LLMStreamEventType.ToolStatus,
      data: {
        status: `Verifying if the query fully satisfies the user's requirement`,
      },
    });
    config.writer?.({
      type: LLMStreamEventType.Log,
      data: `Validating the query semantically.`,
    });
    const useSmartLLM =
      this.config.nodes?.semanticValidatorNode?.useSmartLLM ?? false;
    const llm = useSmartLLM ? this.smartllm : this.cheapllm;
    const chain = RunnableSequence.from([this.prompt, llm]);
    const output = await chain.invoke({
      query: state.sql,
      prompt: state.prompt,
      checks: [
        `<must-follow-rules>`,
        'It is really important that the query follows all the following context information -',
        ...(this.checks ?? []),
        ...this.schemaHelper.getTablesContext(state.schema),
        `</must-follow-rules>`,
      ].join('\n'),
      schema: this.schemaHelper.asString(state.schema),
      feedbacks: await this.getFeedbacks(state),
    });
    const response = stripThinkingTokens(output);

    const lastLine = response.split('\n').pop() ?? '';
    const isValid = lastLine.startsWith('valid');
    if (isValid) {
      return {
        ...state,
        feedbacks: state.feedbacks?.filter(
          // remove interal feedbacks generated by validators
          feedback => !feedback.startsWith('Query Validation Failed'),
        ),
        status: EvaluationResult.Pass,
      };
    } else {
      const reason = lastLine.replace('invalid: ', '').trim();
      config.writer?.({
        type: LLMStreamEventType.Log,
        data: `Query Validation Failed by LLM: ${reason}`,
      });
      return {
        ...state,
        status: EvaluationResult.QueryError,
        feedbacks: [
          ...(state.feedbacks ?? []),
          `Query Validation Failed by LLM: ${reason}`,
        ],
      };
    }
  }

  async getFeedbacks(state: DbQueryState) {
    if (state.feedbacks?.length) {
      const feedbacks = await this.feedbackPrompt.format({
        feedback: state.feedbacks.join('\n'),
      });
      return feedbacks;
    }
    return '';
  }
}
